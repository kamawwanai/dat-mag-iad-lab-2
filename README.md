# dat-mag-iad-lab-2

## Эксперименты с гиперпараметрами

Для оценки влияния гиперпараметров проведены эксперименты с:

- learning rate: `0.01`, `0.05`, `0.1`
- размер скрытого слоя: `hidden_size = 64` и `128`
- фиксированным числом эпох: `20`
- batch size: `64`

### Валидационная точность

![Validation accuracy](docs/val_accuracy.png)

График показывает, что:

- Увеличение **learning rate** с `0.01` до `0.1` заметно ускоряет обучение и приводит к более высокой итоговой точности при тех же 20 эпохах.
- Увеличение **hidden_size** с `64` до `128` даёт стабильный прирост точности на всех lr.
- Лучшие эксперименты (`lr = 0.1`, `hidden_size = 128`) выходят на валидационную точность около **0.92–0.93** уже к 8–10‑й эпохе, после чего кривая выходит на плато.

Такое поведение типично для MLP на MNIST: без свёрток и с MSELoss вместо кросс‑энтропии потолок качества оказывается ниже, чем у современных сверточных моделей

### Train loss

![Train loss](docs/train_loss.png)

Из графика видно, что:

- При `lr = 0.05` и `0.1` train loss очень быстро падает и стабилизируется на уровне ~0.01–0.015.
- У меньшего шага (`lr = 0.01`) убывание loss более плавное, и к 20‑й эпохе модель заметно недообучена по сравнению с более агрессивными lr.
- Модель с `hidden_size = 128` достигает меньшего loss, чем с `64`, что согласуется с более высокой валидационной точностью.
